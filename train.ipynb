{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from datasets.kitti_raw_monosf import KITTI_Raw_KittiSplit_Train, KITTI_Raw_KittiSplit_Valid\n",
    "from models.CNet import CNet\n",
    "from augmentations import Augmentation_SceneFlow\n",
    "\n",
    "from losses import _adaptive_disocc_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "args = {\n",
    "    'batch_size' : 1,\n",
    "    'num_workers' : 2,\n",
    "    'epochs': 1, \n",
    "    'lr': 2e-4,\n",
    "    'momentum': 0.9,\n",
    "    'beta': 0.999,\n",
    "    'weight_decay': 0.0,\n",
    "    'train': True,\n",
    "    'cuda': True,\n",
    "    'debug': False,\n",
    "    'evaluation': False,\n",
    "    'finetuning': False,\n",
    "}\n",
    "\n",
    "data_root = '/external/datasets/kitti_data_jpg/'\n",
    "train_dataset = KITTI_Raw_KittiSplit_Train(args={}, root=data_root)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=True, pin_memory=True)\n",
    "# val_dataset = KITTI_Raw_KittiSplit_Valid(args={}, root=data_root)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72686514\n"
     ]
    }
   ],
   "source": [
    "%autoreload \n",
    "\n",
    "augmentation = Augmentation_SceneFlow(args)\n",
    "model = CNet(args)\n",
    "\n",
    "if args['cuda']:\n",
    "    augmentation = augmentation.cuda()\n",
    "    model = model.cuda()\n",
    "    \n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_params)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=args['lr'], betas=[args['momentum'], args['beta']], weight_decay=args['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 7.92620849609375 Gb\n",
      "free     : 6.1063232421875 Gb\n",
      "used     : 1.81988525390625 Gb\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "import pynvml\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total / (1024**3)} Gb')\n",
    "print(f'free     : {info.free / (1024**3)} Gb')\n",
    "print(f'used     : {info.used / (1024**3)} Gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 iterations took 4.380586862564087s\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "start = time.time()\n",
    "for i, data_dict in enumerate(train_loader):\n",
    "    # Get input and target tensor keys\n",
    "    input_keys = list(filter(lambda x: \"input\" in x, data_dict.keys()))\n",
    "    target_keys = list(filter(lambda x: \"target\" in x, data_dict.keys()))\n",
    "    tensor_keys = input_keys + target_keys\n",
    "\n",
    "    # Possibly transfer to Cuda\n",
    "    if args['cuda']:\n",
    "        for key, value in data_dict.items():\n",
    "            if key in tensor_keys:\n",
    "                data_dict[key] = value.cuda(non_blocking=True)\n",
    "\n",
    "    if augmentation is not None:\n",
    "        with torch.no_grad():\n",
    "            data_dict = augmentation(data_dict)\n",
    "        \n",
    "    x = model(data_dict)\n",
    "    if i == 9:\n",
    "        print(f\"5 iterations took {time.time() - start}s\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0888, 0.8093, 0.5732],\n",
      "          [0.3487, 0.5563, 0.5801],\n",
      "          [0.8364, 0.3219, 0.6261]]]])\n",
      "tensor([[[0.0888, 0.8093, 0.5732],\n",
      "         [0.3487, 0.5563, 0.5801],\n",
      "         [0.8364, 0.3219, 0.6261]]])\n"
     ]
    }
   ],
   "source": [
    "flow_s = torch.rand((1, 2, 3, 3))\n",
    "flow_d = torch.rand((1, 2, 3, 3))\n",
    "flow_cam = torch.rand((1, 2, 3, 3))\n",
    "rigidity_mask = torch.randint(0, 2, (1, 1, 3, 3)).type_as(flow_s)\n",
    "\n",
    "loss_static = torch.norm(flow_s * rigidity_mask, p=2)\n",
    "loss_dynamic = torch.norm(flow_d * (1-rigidity_mask), p=2)\n",
    "loss_static + loss_dynamic\n",
    "\n",
    "torch.norm(flow_cam-flow_s, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 2, 256, 832]), torch.Size([1, 2, 128, 416]), torch.Size([1, 2, 64, 208]), torch.Size([1, 2, 32, 104]), torch.Size([1, 2, 16, 52])]\n"
     ]
    }
   ],
   "source": [
    "from utils.sceneflow_util import projectSceneFlow2Flow\n",
    "\n",
    "# mask loss function\n",
    "\n",
    "# optical flow losses\n",
    "optical_flows = [projectSceneFlow2Flow(data_dict['input_k_l1'], sf, disp) for (sf, disp) in zip(x['flow_f'], x['disp_l1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create consistency loss functions\n",
    "if data_dict:\n",
    "    del data_dict\n",
    "if x:\n",
    "    del x\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, augmentations):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for i, data in enumerate(dataloader):\n",
    "    continue\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f3535e70fcd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(args['epochs']):\n",
    "    loss_one_epoch = train_one_epoch(model, dataloader, optimizer, augmentations=augmentation)\n",
    "    optimizer.zero_grad()\n",
    "    loss_one_epoch.backward()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SF",
   "language": "python",
   "name": "sf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
