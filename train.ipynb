{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 81, 4, 13]) torch.Size([1, 256, 4, 13]) torch.Size([1, 64, 4, 13]) torch.Size([1, 3, 4, 13]) torch.Size([1, 3, 4, 13]) torch.Size([1, 1, 4, 13]) torch.Size([1, 1, 4, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sniradi/envs/env_sf/lib/python3.7/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-753741574d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/env_sf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/sceneflow/CNet/models/CNet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# Left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         output_dict = self.run_pwc(\n\u001b[0;32m--> 245\u001b[0;31m             input_dict, input_dict['input_l1_aug'], input_dict['input_l2_aug'], input_dict['input_k_l1_aug'], input_dict['input_k_l2_aug'])\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# Right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/sceneflow/CNet/models/CNet.py\u001b[0m in \u001b[0;36mrun_pwc\u001b[0;34m(self, input_dict, x1_raw, x2_raw, k1, k2, mono)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;31m# becuase K can be changing when doing augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 x2_warp = self.warping_layer_sf(\n\u001b[0;32m--> 113\u001b[0;31m                     x2, flow_f, disp_l1, k1, input_dict['aug_size'])\n\u001b[0m\u001b[1;32m    114\u001b[0m                 x1_warp = self.warping_layer_sf(\n\u001b[1;32m    115\u001b[0m                     x1, flow_b, disp_l2, k2, input_dict['aug_size'])\n",
      "\u001b[0;32m~/envs/env_sf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/sceneflow/CNet/models/modules_sceneflow.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sceneflow, disp, k1, input_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mlocal_scale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mpts1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk1_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel2pts_ms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_scale\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpts2pixel_ms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk1_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msceneflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/sceneflow/CNet/utils/sceneflow_util.py\u001b[0m in \u001b[0;36mpixel2pts_ms\u001b[0;34m(intrinsic, output_disp, rel_scale)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# pixel2pts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     intrinsic_dp_s = intrinsic_scale(\n\u001b[0;32m---> 89\u001b[0;31m         intrinsic, rel_scale[:, 0], rel_scale[:, 1])\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0moutput_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp2depth_kitti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_disp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintrinsic_dp_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mpts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel2pts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintrinsic_dp_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/sceneflow/CNet/utils/sceneflow_util.py\u001b[0m in \u001b[0;36mintrinsic_scale\u001b[0;34m(intrinsic, scale_y, scale_x)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     r3 = torch.tensor([0., 0., 1.], requires_grad=False).cuda(\n\u001b[0m\u001b[1;32m     80\u001b[0m     ).unsqueeze(0).expand(b, -1)\n\u001b[1;32m     81\u001b[0m     \u001b[0mintrinsic_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from datasets.kitti_raw_monosf import KITTI_Raw_KittiSplit_Train, KITTI_Raw_KittiSplit_Valid\n",
    "from models.CNet import CNet\n",
    "from augmentations import Augmentation_SceneFlow\n",
    "\n",
    "# args\n",
    "args = {\n",
    "    'batch_size' : 1,\n",
    "    'num_workers' : 2,\n",
    "    'epochs': 1, \n",
    "    'lr': 2e-4,\n",
    "    'momentum': 0.9,\n",
    "    'beta': 0.999,\n",
    "    'weight_decay': 0.0,\n",
    "    'train': True\n",
    "}\n",
    "\n",
    "data_root = '/external/datasets/kitti_data_jpg/'\n",
    "train_dataset = KITTI_Raw_KittiSplit_Train(args={}, root=data_root)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=True)\n",
    "val_dataset = KITTI_Raw_KittiSplit_Valid(args={}, root=data_root)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=False)\n",
    "\n",
    "%autoreload \n",
    "\n",
    "augmentation = Augmentation_SceneFlow(args)\n",
    "model = CNet(args)\n",
    "\n",
    "params = model.parameters()\n",
    "\n",
    "optimizer = Adam(params, lr=args['lr'], betas=[args['momentum'], args['beta']], weight_decay=args['weight_decay'])\n",
    "\n",
    "%autoreload\n",
    "for data in train_loader:\n",
    "    x = augmentation(data)\n",
    "    x = model(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, augmentations):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for i, data in enumerate(dataloader):\n",
    "    continue\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f3535e70fcd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(args['epochs']):\n",
    "    loss_one_epoch = train_one_epoch(model, dataloader, optimizer, augmentations=augmentation)\n",
    "    optimizer.zero_grad()\n",
    "    loss_one_epoch.backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337\n",
      "309\n",
      "245\n",
      "213\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, dilation=1, isReLU=True):\n",
    "    if isReLU:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((kernel_size - 1) * dilation) // 2, bias=True),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((kernel_size - 1) * dilation) // 2, bias=True)\n",
    "        )\n",
    "    \n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_chs):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.num_chs = num_chs\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        for _, (ch_in, ch_out) in enumerate(zip(num_chs[:-1], num_chs[1:])):\n",
    "            layer = nn.Sequential(\n",
    "                conv(ch_in, ch_out, stride=2),\n",
    "                conv(ch_out, ch_out)\n",
    "            )\n",
    "            self.convs.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_pyramid = []\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "            feature_pyramid.append(x)\n",
    "\n",
    "        return feature_pyramid[::-1]\n",
    "\n",
    "class MaskNetDecoder(nn.Module):\n",
    "  def __init__(self, in_ch, num_refs=1):\n",
    "    super(MaskNetDecoder, self).__init__()\n",
    "\n",
    "    conv_chs = [256, 256, 128, 64, 32, 16]\n",
    "    self.convs = nn.Sequential(\n",
    "      conv(in_ch,       in_ch,       kernel_size=7),\n",
    "      conv(in_ch,       conv_chs[0], kernel_size=5),\n",
    "      conv(conv_chs[0], conv_chs[1]),\n",
    "      conv(conv_chs[1], conv_chs[2]),\n",
    "      conv(conv_chs[2], conv_chs[3]),\n",
    "      conv(conv_chs[3], conv_chs[4]),\n",
    "      conv(conv_chs[4], conv_chs[5]),\n",
    "    )\n",
    "\n",
    "    self.pred_mask = conv(conv_chs[-1], num_refs)\n",
    "    self.pred_mask_upconv = upconv_transpose(num_refs, num_refs)\n",
    "    self.refine_upconv = conv(num_refs, num_refs)\n",
    "    \n",
    "  def forward(self, pyr_feat, nlevel):\n",
    "    out_conv = self.convs(pyr_feat)\n",
    "    mask = self.pred_mask(out_conv)\n",
    "    pred_mask = nn.functional.sigmoid(mask)\n",
    "    mask_upconv = self.pred_mask_upconv(mask)\n",
    "    pred_mask_upconv = nn.functional.sigmoid(self.refine_upconv(mask_upconv))\n",
    "\n",
    "    return pred_mask, pred_mask_upconv  # (B, num_refs, H, W), (B, num_refs, H*2, W*2)    \n",
    "\n",
    "\n",
    "convs = nn.ModuleList()\n",
    "mask_decoders = nn.ModuleList()\n",
    "\n",
    "num_chs = [3, 32, 64, 96, 128, 192, 256]\n",
    "search_range = 4\n",
    "output_level = 4\n",
    "num_levels   = 7\n",
    "\n",
    "for l, ch in enumerate(num_chs[::-1]):\n",
    "    if l > output_level:\n",
    "                break\n",
    "    if l == 0:\n",
    "        num_ch_in = dim_corr + ch \n",
    "    else:\n",
    "        num_ch_in = dim_corr + ch + 32 + 3 + 1\n",
    "    print(num_ch_in)\n",
    "    \n",
    "x = torch.rand(256, 832, 3).permute(2, 0, 1).unsqueeze(0)\n",
    "feat_extractor = FeatureExtractor(num_chs)\n",
    "pyr = feat_extractor(x)\n",
    "print([_.shape for _ in pyr] + [x.shape])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SF",
   "language": "python",
   "name": "sf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
